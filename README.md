<p align="center"><h2>SENTIMENT ANALYSIS OF VACCINATION TWEETS: UNCOVERING PUBLIC PERCEPTION USING NLP</h2></p>

## Project Overview
This project aimed to analyze public sentiment toward vaccinations by applying natural language processing (NLP) techniques to a dataset of 228,207 tweets sourced from Twitter (now X). The goal was to classify tweets as Positive, Negative, or Neutral, uncover dominant public perceptions, and visualize key insights to inform public health strategies. By leveraging NLP and sentiment analysis, the project sought to provide actionable insights into social media discourse surrounding vaccines, with potential applications in monitoring public opinion and combating misinformation.
## Objectives
* To analyze Public Sentiment on Vaccinations.
* To develop a Robust NLP Pipeline.
* To automate Sentiment Labeling.
* To visualize Sentiment Trends and Key Themes.
* To prepare Data for Machine Learning.
* To handle Large-Scale Social Media Data.
* To uncover Actionable Insights.
## Dataset Description
The vaccination_all_tweets.csv dataset contains 228,207 tweets related to vaccinations, collected to analyze public sentiment using NLP techniques. It includes 16 columns capturing tweet content, user metadata, and engagement metrics, stored in a CSV file. The dataset’s structure, as revealed by df.info(), supports text classification tasks, with the text column as the primary feature and metadata providing contextual insights. Below is the description of each column:
* **Id (int64)**: Unique identifier for each tweet. It is used for tracking and referencing tweets; not a feature for modeling.
* **user_name (object)**: Twitter username or handle of the tweet’s author. It provides user context and could identify influential users but rarely used in sentiment modeling.
* **user_location (object)**: User-reported location (e.g., city, country). It could enable geographic sentiment analysis (e.g., regional vaccine attitudes), but 29% missingness limits utility.
* **user_description (object)**: User’s profile bio, describing their interests or identity. It could provide context (e.g., bios with “doctor” might correlate with pro-vaccine sentiment); usable as secondary text for NLP.
* **user_created (object)**: Date and time the user’s account was created. It Indicates account age; could flag new accounts potentially associated with bots or spam.
* **user_followers (int64)**: Number of followers the user has at the time of the tweet. It indicates user influence; could be a feature if correlated with sentiment or tweet impact.
* **user_friends (int64)**: Number of accounts the user follows. It reflects user engagement; less directly relevant but could indicate social activity.
* **user_favourites (int64)**: Total number of tweets the user has liked. It indicates user activity; marginal relevance for sentiment analysis.
* **user_verified (bool)**: Whether the user’s account is verified (e.g., public figures, organizations). It indicates credibility; could be a categorical feature (True/False) for modeling.
* **date (object)**: Date and time the tweet was posted. It enables temporal analysis (e.g., sentiment trends during vaccine rollouts); convert to datetime for time-series analysis.
* **text (object)**: The full text of the tweet (up to 280 characters). It is the primary feature for NLP; used for sentiment analysis after preprocessing (lowercasing, removing URLs, tokenizing, stemming).
* **hashtags (object)**: Hashtags included in the tweet, often stored as a string or list. Secondary text feature; can be processed like text or used to filter tweets (e.g., #Vaccine vs. #AntiVax).
* **source (object)**: Platform or client used to post the tweet (e.g., “Twitter for iPhone”). Could indicate human vs. bot activity; marginal relevance for sentiment.
* **retweets (int64)**: Number of times the tweet was retweeted. Measures tweet popularity; could be a feature if correlated with sentiment or influence.
* **favorites (int64)**: Number of times the tweet was liked. Similar to retweets; indicates engagement.
* **is_retweet (bool)**: Whether the tweet is a retweet (True) or original (False). Could filter out retweets to focus on original opinions or use as a feature to study retweeting behavior.
## Dataset Preprocessing
* **Lowercasing Text**: It converts all characters in the text column to lowercase to ensure consistency and reduce vocabulary size. In NLP, case sensitivity can treat “Vaccine” and “vaccine” as different words, inflating the feature space. Lowercasing standardizes text, ensuring that variations of the same word are treated identically during tokenization and feature extraction.
* **Removing URLs**: It eliminates URLs (e.g., https://t.co/abc) from tweets, as they contribute noise and lack semantic value for sentiment analysis. Social media text often includes links that are irrelevant to sentiment or topic analysis. Removing URLs reduces noise and prevents tokenization of meaningless strings (e.g., “t.co”).
* **Removing Mentions and Hashtags**: Removes Twitter mentions (e.g., @username) and hashtags (e.g., #Vaccine) to focus on core text content. Mentions and hashtags are metadata that may not contribute to sentiment (e.g., @CDCgov is neutral). Removing them simplifies text while preserving meaning, though hashtags can sometimes carry sentiment (e.g., #AntiVax).
* **Removing Punctuation and Special Characters**: It eliminates punctuation (e.g., !, ?) and special characters to focus on words, reducing noise and vocabulary size. Punctuation rarely contributes to sentiment in lexicon-based methods like TextBlob and can complicate tokenization. Removing non-alphanumeric characters simplifies text for feature extraction (e.g., TF-IDF).
* **Tokenization**: Splits text into individual words (tokens) to enable word-level processing like stop words removal and stemming. Tokenization is a foundational NLP step, breaking text into units (words, punctuation) for analysis. nltk.word_tokenize uses a sophisticated algorithm to handle contractions, punctuation, and special cases, outperforming simple splitting (e.g., text.split()).
* **Stop Words Removal**: Filters out common English words (e.g., “the”, “is”) that carry little semantic value, reducing noise and feature dimensionality. Stop words are frequent but low-information words that can dominate feature vectors (e.g., in TF-IDF) without contributing to sentiment. Removing them focuses analysis on meaningful terms (e.g., “vaccine”, “safe”).
* **Joining Tokens into a String**: Combines filtered tokens back into a space-separated string for compatibility with downstream tasks like TextBlob or TfidfVectorizer. After token-level processing, many NLP tools expect text as a single string. Joining ensures the processed text is in a format suitable for sentiment analysis or feature extraction.
* **Stemming (Attempted, but Flawed)**: Intended to reduce words to their root form (e.g., “vaccinated” → “vaccin”) to group related terms and reduce vocabulary size. Stemming applies rule-based truncation to normalize words (e.g., Porter Stemming algorithm). Reduces feature dimensionality and improves model generalization by treating variants as one term.
* **Handling Missing Values in Metadata**: Addresses missing values in columns like user_location, hashtags, and user_description to ensure data integrity for analysis or modeling. Missing values can cause errors in processing or bias analysis if not handled. Common strategies include imputation (e.g., filling with “Unknown” or empty strings) or exclusion, depending on the column’s role.
* **Creating New Columns for Processed Text and Sentiment**: Stores processed text, polarity scores, and sentiment labels in new columns for analysis and visualization. Adding columns preserves raw data while enabling downstream tasks (e.g., sentiment analysis, machine learning). Storing processed text avoids repeated computation, improving efficiency.
## Exploratory Data Analysis
* **Sentiment Distribution Count Plot**: The count plot visualizes the number of tweets in each sentiment category (Positive, Negative, Neutral) as determined by the TextBlob sentiment analysis. It helps assess the balance of sentiments in the dataset and identify dominant public attitudes toward vaccinations. Count plots (or bar plots for categorical data) are a fundamental visualization in data science, used to show the frequency of categories in a dataset. They reveal class balance, which is critical for text classification tasks, as imbalanced classes (e.g., many Positive tweets, few Negative) can affect model performance or interpretation. In social media analysis, understanding sentiment distribution provides insights into public opinion (e.g., widespread vaccine support vs. skepticism). This informs applications like public health campaigns or misinformation detection.
* **Word Cloud for Positive Tweets**: The word cloud visualizes the most frequent words in tweets labeled as Positive, highlighting key themes and terms associated with supportive vaccination sentiments (e.g., “safe”, “protected”). Word clouds are a popular visualization for text data, where word size or length reflects frequency or importance. They provide a quick, intuitive way to identify dominant terms in a corpus, useful for EDA and presentation. Focusing on Positive tweets isolates optimistic language, revealing what drives vaccine support (e.g., trust in science, personal experiences). The WordCloud library generates word clouds by counting word frequencies and mapping them to visual sizes, with customizable parameters (e.g., max_words, width, height).
